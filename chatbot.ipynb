{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 3392\n",
      "Sample chunk:\n",
      "\n",
      "The GALE\n",
      "ENCYCLOPEDIA\n",
      "of MEDICINE\n",
      "SECOND EDITION\n",
      "\n",
      "The GALE\n",
      "ENCYCLOPEDIA\n",
      "of MEDICINE\n",
      "SECOND EDITION\n",
      "J A C Q U E L I N E  L .  L O N G E ,  E D I T O R\n",
      "D E I R D R E  S .  B L A N C H F I E L D ,  A S S O C I AT E  E D I T O R\n",
      "V O L U M E\n",
      "A-B\n",
      "1...\n"
     ]
    }
   ],
   "source": [
    "# ✅ Cell 2: Load and Chunk PDF using PyMuPDF + LangChain\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Load the PDF and extract text\n",
    "def load_pdf(file_path: str) -> list:\n",
    "    doc = fitz.open(file_path)\n",
    "    text_pages = [page.get_text() for page in doc]\n",
    "    doc.close()\n",
    "    full_text = \"\\n\".join(text_pages)\n",
    "    return full_text\n",
    "\n",
    "# Chunk text into overlapping pieces\n",
    "def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> list[Document]:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    chunks = splitter.create_documents([text])\n",
    "    return chunks\n",
    "\n",
    "# Load and chunk the document\n",
    "pdf_path = \"Medical_book.pdf\"\n",
    "raw_text = load_pdf(pdf_path)\n",
    "documents = chunk_text(raw_text)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Total chunks created: {len(documents)}\")\n",
    "print(f\"Sample chunk:\\n\\n{documents[0].page_content[:500]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1913ff8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aakas\\Downloads\\Mathes\\Medical Chatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\aakas\\Downloads\\Mathes\\Medical Chatbot\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\aakas\\.cache\\huggingface\\hub\\models--BAAI--bge-large-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Batches: 100%|██████████| 106/106 [40:00<00:00, 22.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated 3392 embeddings of dimension 1024\n"
     ]
    }
   ],
   "source": [
    "# ✅ Cell 4: Generate embeddings using bge-large-en-v1.5 (HuggingFace)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load the embedding model (downloaded from HuggingFace Hub)\n",
    "model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "# Extract raw text content from LangChain Document objects\n",
    "texts = [doc.page_content for doc in documents]\n",
    "\n",
    "# Generate embeddings for all chunks\n",
    "embeddings = model.encode(texts, show_progress_bar=True, batch_size=32, normalize_embeddings=True)\n",
    "\n",
    "# Convert to list of vectors (if needed later for storage)\n",
    "embeddings = embeddings.tolist()\n",
    "\n",
    "print(f\"✅ Generated {len(embeddings)} embeddings of dimension {len(embeddings[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449ec927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created index: medical-chatbot-index\n"
     ]
    }
   ],
   "source": [
    "# ✅ Cell 6: Pinecone (v3.x) initialization and index creation\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Create Pinecone client instance\n",
    "pc = Pinecone(api_key=\"Enter_Your_Pinecone_API_Key_Here\"\")\n",
    "\n",
    "# Check and create index if it doesn't exist\n",
    "if INDEX_NAME not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=len(embeddings[0]),\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    ")\n",
    "    )\n",
    "    print(f\"✅ Created index: {INDEX_NAME}\")\n",
    "else:\n",
    "    print(f\"✅ Index already exists: {INDEX_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fe32ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded batch 1\n",
      "✅ Uploaded batch 2\n",
      "✅ Uploaded batch 3\n",
      "✅ Uploaded batch 4\n",
      "✅ Uploaded batch 5\n",
      "✅ Uploaded batch 6\n",
      "✅ Uploaded batch 7\n",
      "✅ Uploaded batch 8\n",
      "✅ Uploaded batch 9\n",
      "✅ Uploaded batch 10\n",
      "✅ Uploaded batch 11\n",
      "✅ Uploaded batch 12\n",
      "✅ Uploaded batch 13\n",
      "✅ Uploaded batch 14\n",
      "✅ Uploaded batch 15\n",
      "✅ Uploaded batch 16\n",
      "✅ Uploaded batch 17\n",
      "✅ Uploaded batch 18\n",
      "✅ Uploaded batch 19\n",
      "✅ Uploaded batch 20\n",
      "✅ Uploaded batch 21\n",
      "✅ Uploaded batch 22\n",
      "✅ Uploaded batch 23\n",
      "✅ Uploaded batch 24\n",
      "✅ Uploaded batch 25\n",
      "✅ Uploaded batch 26\n",
      "✅ Uploaded batch 27\n",
      "✅ Uploaded batch 28\n",
      "✅ Uploaded batch 29\n",
      "✅ Uploaded batch 30\n",
      "✅ Uploaded batch 31\n",
      "✅ Uploaded batch 32\n",
      "✅ Uploaded batch 33\n",
      "✅ Uploaded batch 34\n",
      "✅ All embeddings uploaded to Pinecone!\n"
     ]
    }
   ],
   "source": [
    "# ✅ Cell 7: Upload embeddings and metadata to Pinecone\n",
    "\n",
    "# Connect to your index\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "# Prepare batch data\n",
    "batch_size = 100  # Pinecone recommends batching for large uploads\n",
    "vectors = []\n",
    "\n",
    "for i, (text, vector) in enumerate(zip(texts, embeddings)):\n",
    "    vectors.append({\n",
    "        \"id\": f\"chunk-{i}\",\n",
    "        \"values\": vector,\n",
    "        \"metadata\": {\n",
    "            \"text\": text\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Upload in batches\n",
    "for i in range(0, len(vectors), batch_size):\n",
    "    batch = vectors[i:i+batch_size]\n",
    "    index.upsert(vectors=batch)\n",
    "    print(f\"✅ Uploaded batch {i // batch_size + 1}\")\n",
    "\n",
    "print(\"✅ All embeddings uploaded to Pinecone!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af68471c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gemini 1.5 Pro loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Cell 9: Initialize Gemini Pro from Google Generative AI\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "# 🔐 Replace with your actual API key\n",
    "GEMINI_API_KEY = \"Enter_Your_Gemini_API_Key_Here\"\n",
    "\n",
    "# Configure Gemini\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Load Gemini Pro model\n",
    "gemini_model = genai.GenerativeModel(model_name=\"models/gemini-1.5-flash\")\n",
    "\n",
    "\n",
    "print(\"✅ Gemini 1.5 Pro loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80637cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Cell 10: Embed user query and retrieve top-k similar chunks from Pinecone\n",
    "\n",
    "def search_similar_chunks(query: str, top_k: int = 5):\n",
    "    # Embed the query using the same model\n",
    "    query_vector = model.encode(query, normalize_embeddings=True).tolist()\n",
    "\n",
    "    # Query Pinecone index\n",
    "    results = index.query(\n",
    "        vector=query_vector,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    # Extract retrieved texts\n",
    "    retrieved_chunks = [match['metadata']['text'] for match in results['matches']]\n",
    "    return retrieved_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "163acc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Cell 11: Generate medical answer using Gemini and retrieved context\n",
    "\n",
    "def ask_medical_question(question: str, top_k: int = 5):\n",
    "    # 1. Retrieve relevant chunks\n",
    "    context_chunks = search_similar_chunks(question, top_k=top_k)\n",
    "    context_text = \"\\n\\n\".join(context_chunks)\n",
    "\n",
    "    # 2. Prepare prompt for Gemini\n",
    "    prompt = f\"\"\"You are a helpful medical assistant. Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # 3. Generate answer with Gemini Pro\n",
    "    response = gemini_model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# ▶️ Example usage:\n",
    "# print(ask_medical_question(\"What are the symptoms of diabetes?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0a95848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided text, symptoms of Type I diabetes mellitus include fatigue and abnormally high levels of glucose in the blood (hyperglycemia).  The text also mentions that if diabetes is left untreated, it can damage or cause failure of the eyes, kidneys, nerves, heart, blood vessels, and other body organs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ask_medical_question(\"What are the symptoms of diabetes?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1458f574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
